{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# for running on wayland, ignore on other platforms #\n",
    "import os                                           #\n",
    "os.environ[\"XDG_SESSION_TYPE\"] = \"xcb\"              #\n",
    "#####################################################\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.7ms\n",
      "Speed: 1.4ms preprocess, 10.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9.1ms\n",
      "Speed: 1.1ms preprocess, 9.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.6ms\n",
      "Speed: 1.4ms preprocess, 11.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 320x640 2 persons, 8.5ms\n",
      "Speed: 1.1ms preprocess, 8.5ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 352x640 2 persons, 8.8ms\n",
      "Speed: 1.1ms preprocess, 8.8ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.9ms preprocess, 10.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.5ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.4ms\n",
      "Speed: 1.5ms preprocess, 14.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 11.5ms\n",
      "Speed: 1.4ms preprocess, 11.5ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.5ms\n",
      "Speed: 2.2ms preprocess, 13.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 288x640 2 persons, 7.2ms\n",
      "Speed: 0.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.0ms\n",
      "Speed: 1.2ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.1ms\n",
      "Speed: 2.7ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.9ms\n",
      "Speed: 1.2ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.6ms\n",
      "Speed: 1.4ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 5 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.9ms\n",
      "Speed: 1.3ms preprocess, 12.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.3ms\n",
      "Speed: 1.8ms preprocess, 13.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.1ms\n",
      "Speed: 1.1ms preprocess, 10.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.1ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 10.4ms\n",
      "Speed: 1.0ms preprocess, 10.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 1.1ms preprocess, 10.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.2ms\n",
      "Speed: 1.3ms preprocess, 13.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.4ms\n",
      "Speed: 1.4ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 11.0ms\n",
      "Speed: 1.6ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.9ms\n",
      "Speed: 1.2ms preprocess, 12.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 1.1ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.1ms preprocess, 9.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.1ms\n",
      "Speed: 2.0ms preprocess, 13.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.9ms\n",
      "Speed: 1.3ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.9ms\n",
      "Speed: 1.6ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.4ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 17.2ms\n",
      "Speed: 3.1ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 11.0ms\n",
      "Speed: 2.1ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 3 persons, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 10.0ms\n",
      "Speed: 1.2ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.2ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 7.3ms\n",
      "Speed: 1.2ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.9ms\n",
      "Speed: 1.4ms preprocess, 12.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 9.4ms\n",
      "Speed: 0.9ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 11.2ms\n",
      "Speed: 1.9ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.2ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.0ms\n",
      "Speed: 1.1ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 12.3ms\n",
      "Speed: 1.2ms preprocess, 12.3ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.4ms\n",
      "Speed: 1.1ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.7ms\n",
      "Speed: 1.1ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 12.0ms\n",
      "Speed: 1.2ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.4ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.7ms\n",
      "Speed: 1.3ms preprocess, 14.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.0ms\n",
      "Speed: 1.2ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 11.4ms\n",
      "Speed: 1.2ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.7ms\n",
      "Speed: 1.1ms preprocess, 10.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.2ms\n",
      "Speed: 1.1ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 2 persons, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x512 1 person, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x416 1 person, 9.4ms\n",
      "Speed: 1.0ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 1 person, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.0ms\n",
      "Speed: 1.6ms preprocess, 13.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.7ms\n",
      "Speed: 1.5ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.0ms\n",
      "Speed: 1.9ms preprocess, 13.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.4ms\n",
      "Speed: 1.4ms preprocess, 10.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 1.2ms preprocess, 10.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.1ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.0ms preprocess, 9.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.1ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.8ms\n",
      "Speed: 2.4ms preprocess, 12.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 10.2ms\n",
      "Speed: 1.2ms preprocess, 10.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.1ms\n",
      "Speed: 1.5ms preprocess, 13.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.7ms\n",
      "Speed: 1.8ms preprocess, 12.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.7ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.5ms\n",
      "Speed: 1.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.5ms\n",
      "Speed: 1.1ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 11.3ms\n",
      "Speed: 1.1ms preprocess, 11.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.3ms\n",
      "Speed: 1.1ms preprocess, 8.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 1.1ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.2ms\n",
      "Speed: 1.0ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.9ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.4ms\n",
      "Speed: 1.3ms preprocess, 13.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 17.0ms\n",
      "Speed: 2.8ms preprocess, 17.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.7ms\n",
      "Speed: 1.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 10.6ms\n",
      "Speed: 1.2ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.6ms\n",
      "Speed: 1.1ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.1ms\n",
      "Speed: 1.1ms preprocess, 10.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 7.3ms\n",
      "Speed: 1.0ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 9 persons, 7.4ms\n",
      "Speed: 1.2ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 7.2ms\n",
      "Speed: 1.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.1ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 4 persons, 8.0ms\n",
      "Speed: 0.9ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.7ms\n",
      "Speed: 1.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 5 persons, 9.6ms\n",
      "Speed: 1.3ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.3ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.2ms\n",
      "Speed: 1.2ms preprocess, 8.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 4 persons, 7.6ms\n",
      "Speed: 0.9ms preprocess, 7.6ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.8ms\n",
      "Speed: 1.1ms preprocess, 7.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.3ms\n",
      "Speed: 1.2ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 5 persons, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.9ms\n",
      "Speed: 1.0ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 10.6ms\n",
      "Speed: 1.1ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 6 persons, 10.1ms\n",
      "Speed: 2.8ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 11 persons, 7.8ms\n",
      "Speed: 1.0ms preprocess, 7.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 2 persons, 9.1ms\n",
      "Speed: 2.6ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 9.2ms\n",
      "Speed: 1.0ms preprocess, 9.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 7 persons, 9.7ms\n",
      "Speed: 1.0ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 8.4ms\n",
      "Speed: 0.9ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 1 person, 11.7ms\n",
      "Speed: 1.2ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 7.2ms\n",
      "Speed: 0.9ms preprocess, 7.2ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 480x640 5 persons, 9.9ms\n",
      "Speed: 1.5ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 256x640 6 persons, 6.0ms\n",
      "Speed: 0.9ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.6ms\n",
      "Speed: 1.4ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.6ms\n",
      "Speed: 1.1ms preprocess, 9.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 1.1ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.9ms\n",
      "Speed: 1.0ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.1ms\n",
      "Speed: 4.1ms preprocess, 10.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.9ms\n",
      "Speed: 1.8ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.1ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.8ms\n",
      "Speed: 1.0ms preprocess, 8.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 18 persons, 11.8ms\n",
      "Speed: 1.2ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 1 person, 12.7ms\n",
      "Speed: 1.9ms preprocess, 12.7ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 320x640 1 person, 9.3ms\n",
      "Speed: 1.1ms preprocess, 9.3ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 0.9ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 12.2ms\n",
      "Speed: 1.2ms preprocess, 12.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 7 persons, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 14 persons, 11.4ms\n",
      "Speed: 1.3ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 288x640 12 persons, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 480x640 6 persons, 11.9ms\n",
      "Speed: 1.2ms preprocess, 11.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 5 persons, 12.3ms\n",
      "Speed: 1.7ms preprocess, 12.3ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 352x640 14 persons, 8.6ms\n",
      "Speed: 1.1ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 352x640 12 persons, 8.6ms\n",
      "Speed: 1.1ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 5 persons, 8.1ms\n",
      "Speed: 0.9ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>...</th>\n",
       "      <th>y_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>y_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>y_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>y_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>y_10</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>348</td>\n",
       "      <td>678</td>\n",
       "      <td>499</td>\n",
       "      <td>384</td>\n",
       "      <td>509</td>\n",
       "      <td>575</td>\n",
       "      <td>194</td>\n",
       "      <td>509</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>235</td>\n",
       "      <td>528</td>\n",
       "      <td>190</td>\n",
       "      <td>450</td>\n",
       "      <td>188</td>\n",
       "      <td>586</td>\n",
       "      <td>195</td>\n",
       "      <td>399</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>393</td>\n",
       "      <td>822</td>\n",
       "      <td>479</td>\n",
       "      <td>56</td>\n",
       "      <td>501</td>\n",
       "      <td>736</td>\n",
       "      <td>267</td>\n",
       "      <td>251</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>166</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>187</td>\n",
       "      <td>74</td>\n",
       "      <td>192</td>\n",
       "      <td>67</td>\n",
       "      <td>153</td>\n",
       "      <td>78</td>\n",
       "      <td>156</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>111</td>\n",
       "      <td>145</td>\n",
       "      <td>87</td>\n",
       "      <td>106</td>\n",
       "      <td>88</td>\n",
       "      <td>166</td>\n",
       "      <td>106</td>\n",
       "      <td>75</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>201</td>\n",
       "      <td>124</td>\n",
       "      <td>207</td>\n",
       "      <td>117</td>\n",
       "      <td>195</td>\n",
       "      <td>120</td>\n",
       "      <td>217</td>\n",
       "      <td>120</td>\n",
       "      <td>189</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>211</td>\n",
       "      <td>41</td>\n",
       "      <td>204</td>\n",
       "      <td>34</td>\n",
       "      <td>206</td>\n",
       "      <td>45</td>\n",
       "      <td>201</td>\n",
       "      <td>23</td>\n",
       "      <td>205</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>251</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>160</td>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>152</td>\n",
       "      <td>155</td>\n",
       "      <td>150</td>\n",
       "      <td>178</td>\n",
       "      <td>154</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>32</td>\n",
       "      <td>123</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "      <td>117</td>\n",
       "      <td>120</td>\n",
       "      <td>134</td>\n",
       "      <td>132</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_0  y_0  x_1  y_1  x_2  y_2  x_3  y_3  x_4  y_4  ...  y_6  x_7  y_7  \\\n",
       "0    478  163    0    0  440  140    0    0  334  189  ...  348  678  499   \n",
       "1    488  235  528  190  450  188  586  195  399  188  ...  393  822  479   \n",
       "2    169   43  166   38    0    0  171   27    0    0  ...   33  187   74   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4    130  111  145   87  106   88  166  106   75  111  ...  227    0    0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "423  201  124  207  117  195  120  217  120  189  126  ...  151    0    0   \n",
       "424  211   41  204   34  206   45  201   23  205   49  ...   39  216    2   \n",
       "425  160  157  167  152  155  150  178  154  147  149  ...    0    0    0   \n",
       "426   88   82   95   86   93   76    0    0   93   68  ...   57   32  123   \n",
       "427  120  128  128  127  117  120  134  132  107  116  ...  128    0    0   \n",
       "\n",
       "     x_8  y_8  x_9  y_9  x_10  y_10  class  \n",
       "0    384  509  575  194   509   184      1  \n",
       "1     56  501  736  267   251   252      1  \n",
       "2    192   67  153   78   156    77      1  \n",
       "3      0    0    0    0     0     0      1  \n",
       "4      0    0    0    0     0     0      1  \n",
       "..   ...  ...  ...  ...   ...   ...    ...  \n",
       "423    0    0    0    0     0     0      0  \n",
       "424  251   37    0    0   295    30      0  \n",
       "425    0    0    0    0     0     0      0  \n",
       "426   24   43   18  104     6    38      0  \n",
       "427    0    0    0    0     0     0      0  \n",
       "\n",
       "[428 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataframe generation and class attribute addition\n",
    "\n",
    "pose_model = YOLO('yolov8s-pose.pt')\n",
    "\n",
    "main_dir = 'dataset/train/'\n",
    "\n",
    "position_df = pd.DataFrame()\n",
    "\n",
    "for class_name in os.listdir(main_dir):\n",
    "    class_path = os.path.join(main_dir, class_name)\n",
    "    \n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(class_path, filename)\n",
    "                frame = cv2.imread(image_path)\n",
    "\n",
    "                results = pose_model(frame, device=0)\n",
    "\n",
    "                x_values = []\n",
    "                y_values = []\n",
    "                data = {}\n",
    "                img_df = pd.DataFrame()\n",
    "\n",
    "                for result in results:\n",
    "                    kpts = result.keypoints\n",
    "                    upperbody_kpts = 11\n",
    "\n",
    "                    for person in range(len(kpts)):\n",
    "                        for kp_index in range(upperbody_kpts):\n",
    "                            keypoint = kpts.xy[person, kp_index]\n",
    "                            x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "\n",
    "                            x_values.append(x)\n",
    "                            y_values.append(y)\n",
    "\n",
    "                        for i in range(len(x_values)):\n",
    "                            data[f'x_{i}'] = x_values[i]\n",
    "                            data[f'y_{i}'] = y_values[i]\n",
    "\n",
    "                        x_values = []\n",
    "                        y_values = []\n",
    "\n",
    "                        # Add a column for class (0 for \"healthy\", 1 for \"risk\")\n",
    "                        data['class'] = 0 if class_name == 'healthy' else 1\n",
    "\n",
    "                        temp_df = pd.DataFrame([data])\n",
    "                        img_df = pd.concat([img_df, temp_df], ignore_index=True)\n",
    "                        temp_df = {}\n",
    "\n",
    "                # Concatenate per-image DataFrame to the main DataFrame\n",
    "                position_df = pd.concat([position_df, img_df], ignore_index=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(position_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class BodyLandmarksDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "def get_class_list(df):\n",
    "    # Ensure the DataFrame is not empty\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Get the last column name\n",
    "    last_attribute = df.columns[-1]\n",
    "\n",
    "    # Extract the values of the last (class) attribute and enter into a list\n",
    "    values_list = df[last_attribute].tolist()\n",
    "\n",
    "    return values_list\n",
    "\n",
    "# 11x2 (x, y) body landmarks (22 features) for each sample\n",
    "y = get_class_list(position_df)  # Output labels\n",
    "position_df.drop(position_df.columns[-1], axis=1, inplace=True)\n",
    "X = position_df  # Position (pandas dataframe)\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test sets using PyTorch's native functionality\n",
    "dataset = BodyLandmarksDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 22  # 11x2 coordinates\n",
    "output_size = 2  # Number of output classes\n",
    "hidden_size = 64\n",
    "\n",
    "classifier_model = EmotionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        print(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier_model(inputs.view(-1, input_size))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the test set\n",
    "classifier_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = classifier_model(inputs.view(-1, input_size))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
