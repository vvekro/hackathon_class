{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# for running on wayland, ignore on other platforms #\n",
    "import os                                           #\n",
    "os.environ[\"XDG_SESSION_TYPE\"] = \"xcb\"              #\n",
    "#####################################################\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model output\n",
    "\n",
    "model = YOLO('yolov8n-pose.pt')\n",
    "image_path = 'bus.jpg'\n",
    "frame = cv2.imread(image_path)\n",
    "results = model(frame, device = 0)\n",
    "for result in results:\n",
    "    #print(result.keypoints.data)\n",
    "    kpts = result.keypoints\n",
    "    # first 10 keypoints are needed, legs can be ignored\n",
    "    upperbody_kpts = 11\n",
    "\n",
    "    count = 0\n",
    "    for i in range(upperbody_kpts):\n",
    "        keypoint = kpts.xy[1, i]\n",
    "        x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "        cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "        print(f\"x{count} value: {x} y{count} value: {y}\")\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 2 persons, 111.9ms\n",
      "Speed: 3.5ms preprocess, 111.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>...</th>\n",
       "      <th>x_6</th>\n",
       "      <th>y_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>y_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>y_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>y_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>y_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>335</td>\n",
       "      <td>70</td>\n",
       "      <td>339</td>\n",
       "      <td>64</td>\n",
       "      <td>333</td>\n",
       "      <td>65</td>\n",
       "      <td>358</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>330</td>\n",
       "      <td>104</td>\n",
       "      <td>406</td>\n",
       "      <td>131</td>\n",
       "      <td>310</td>\n",
       "      <td>146</td>\n",
       "      <td>398</td>\n",
       "      <td>156</td>\n",
       "      <td>329</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>77</td>\n",
       "      <td>259</td>\n",
       "      <td>71</td>\n",
       "      <td>251</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>210</td>\n",
       "      <td>115</td>\n",
       "      <td>263</td>\n",
       "      <td>154</td>\n",
       "      <td>190</td>\n",
       "      <td>156</td>\n",
       "      <td>273</td>\n",
       "      <td>158</td>\n",
       "      <td>221</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   x_0  y_0  x_1  y_1  x_2  y_2  x_3  y_3  x_4  y_4  ...  x_6  y_6  x_7  y_7  \\\n",
       "0  335   70  339   64  333   65  358   64    0    0  ...  330  104  406  131   \n",
       "1  258   77  259   71  251   73    0    0  233   77  ...  210  115  263  154   \n",
       "\n",
       "   x_8  y_8  x_9  y_9  x_10  y_10  \n",
       "0  310  146  398  156   329   157  \n",
       "1  190  156  273  158   221   145  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pandas write test\n",
    "\n",
    "model = YOLO('yolov8s-pose.pt')\n",
    "image_path = 'practice_imgs/tworun.jpg'\n",
    "frame = cv2.imread(image_path)\n",
    "results = model(frame)\n",
    "\n",
    "x_values = []\n",
    "y_values = []\n",
    "data ={}\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for result in results:\n",
    "    kpts = result.keypoints\n",
    "    # first 11 keypoints are needed, legs can be ignored\n",
    "    upperbody_kpts = 11\n",
    "\n",
    "    for person in range(len(kpts)):\n",
    "        for kp_index in range(upperbody_kpts):\n",
    "            keypoint = kpts.xy[person, kp_index]\n",
    "            x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "\n",
    "            x_values.append(x)\n",
    "            y_values.append(y)\n",
    "\n",
    "        for i in range(len(x_values)):\n",
    "            data[f'x_{i}'] = x_values[i]\n",
    "            data[f'y_{i}'] = y_values[i]\n",
    "\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        \n",
    "        temp_df = pd.DataFrame([data])\n",
    "        df = pd.concat([df,temp_df], ignore_index=True)\n",
    "        temp_df={}\n",
    "\n",
    "\n",
    "# for i in range(len(x_values)):\n",
    "#     data[f'x_{i}'] = x_values[i]\n",
    "#     data[f'y_{i}'] = y_values[i]\n",
    "\n",
    "# df = pd.DataFrame([data])\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 11.4ms\n",
      "Speed: 1.6ms preprocess, 11.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 9 persons, 8.8ms\n",
      "Speed: 1.2ms preprocess, 8.8ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.8ms\n",
      "Speed: 2.0ms preprocess, 10.8ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 8.5ms\n",
      "Speed: 1.2ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.1ms\n",
      "Speed: 1.3ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 4 persons, 9.1ms\n",
      "Speed: 0.9ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 5 persons, 11.6ms\n",
      "Speed: 1.5ms preprocess, 11.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.6ms\n",
      "Speed: 1.8ms preprocess, 10.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.6ms\n",
      "Speed: 1.1ms preprocess, 10.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 11 persons, 9.2ms\n",
      "Speed: 1.1ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 4 persons, 8.9ms\n",
      "Speed: 0.8ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.1ms\n",
      "Speed: 1.0ms preprocess, 9.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.1ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 5 persons, 11.9ms\n",
      "Speed: 1.3ms preprocess, 11.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.1ms\n",
      "Speed: 1.0ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 10.6ms\n",
      "Speed: 1.4ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 10.9ms\n",
      "Speed: 1.1ms preprocess, 10.9ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 6 persons, 11.1ms\n",
      "Speed: 1.2ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 2 persons, 10.8ms\n",
      "Speed: 2.6ms preprocess, 10.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 11.4ms\n",
      "Speed: 1.7ms preprocess, 11.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.2ms\n",
      "Speed: 1.1ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 7 persons, 11.2ms\n",
      "Speed: 1.0ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 9.3ms\n",
      "Speed: 1.0ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 1 person, 12.8ms\n",
      "Speed: 1.4ms preprocess, 12.8ms inference, 0.7ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.6ms\n",
      "Speed: 1.9ms preprocess, 11.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 8.9ms\n",
      "Speed: 1.1ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 480x640 5 persons, 12.0ms\n",
      "Speed: 2.2ms preprocess, 12.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.1ms\n",
      "Speed: 1.2ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 256x640 6 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.8ms\n",
      "Speed: 1.1ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.7ms\n",
      "Speed: 1.6ms preprocess, 11.7ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.8ms\n",
      "Speed: 1.2ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 11.5ms\n",
      "Speed: 1.2ms preprocess, 11.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.0ms\n",
      "Speed: 1.7ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.5ms\n",
      "Speed: 1.1ms preprocess, 11.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 11.6ms\n",
      "Speed: 2.6ms preprocess, 11.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.3ms\n",
      "Speed: 1.8ms preprocess, 14.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.9ms\n",
      "Speed: 1.7ms preprocess, 10.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.4ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.7ms\n",
      "Speed: 1.8ms preprocess, 10.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 0.8ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.6ms\n",
      "Speed: 1.0ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 18 persons, 11.1ms\n",
      "Speed: 1.2ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 1 person, 12.5ms\n",
      "Speed: 1.8ms preprocess, 12.5ms inference, 0.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.1ms\n",
      "Speed: 3.1ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 320x640 1 person, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 0.9ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 13.4ms\n",
      "Speed: 1.7ms preprocess, 13.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 7 persons, 11.5ms\n",
      "Speed: 2.0ms preprocess, 11.5ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 14 persons, 10.8ms\n",
      "Speed: 1.1ms preprocess, 10.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 288x640 12 persons, 7.5ms\n",
      "Speed: 1.1ms preprocess, 7.5ms inference, 0.8ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 480x640 6 persons, 11.7ms\n",
      "Speed: 1.6ms preprocess, 11.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 5 persons, 12.8ms\n",
      "Speed: 2.2ms preprocess, 12.8ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 352x640 14 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.1ms\n",
      "Speed: 2.0ms preprocess, 11.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 352x640 12 persons, 8.7ms\n",
      "Speed: 1.2ms preprocess, 8.7ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 5 persons, 8.1ms\n",
      "Speed: 1.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>...</th>\n",
       "      <th>x_6</th>\n",
       "      <th>y_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>y_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>y_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>y_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>y_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>632</td>\n",
       "      <td>295</td>\n",
       "      <td>711</td>\n",
       "      <td>254</td>\n",
       "      <td>602</td>\n",
       "      <td>226</td>\n",
       "      <td>798</td>\n",
       "      <td>311</td>\n",
       "      <td>552</td>\n",
       "      <td>239</td>\n",
       "      <td>...</td>\n",
       "      <td>458</td>\n",
       "      <td>499</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>621</td>\n",
       "      <td>517</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>549</td>\n",
       "      <td>818</td>\n",
       "      <td>585</td>\n",
       "      <td>819</td>\n",
       "      <td>...</td>\n",
       "      <td>658</td>\n",
       "      <td>878</td>\n",
       "      <td>410</td>\n",
       "      <td>903</td>\n",
       "      <td>754</td>\n",
       "      <td>839</td>\n",
       "      <td>346</td>\n",
       "      <td>848</td>\n",
       "      <td>812</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2613</td>\n",
       "      <td>590</td>\n",
       "      <td>...</td>\n",
       "      <td>2659</td>\n",
       "      <td>627</td>\n",
       "      <td>2421</td>\n",
       "      <td>564</td>\n",
       "      <td>2760</td>\n",
       "      <td>551</td>\n",
       "      <td>2380</td>\n",
       "      <td>502</td>\n",
       "      <td>2824</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2095</td>\n",
       "      <td>481</td>\n",
       "      <td>2110</td>\n",
       "      <td>464</td>\n",
       "      <td>2084</td>\n",
       "      <td>471</td>\n",
       "      <td>2136</td>\n",
       "      <td>470</td>\n",
       "      <td>2070</td>\n",
       "      <td>485</td>\n",
       "      <td>...</td>\n",
       "      <td>2042</td>\n",
       "      <td>548</td>\n",
       "      <td>2267</td>\n",
       "      <td>453</td>\n",
       "      <td>1937</td>\n",
       "      <td>460</td>\n",
       "      <td>2296</td>\n",
       "      <td>356</td>\n",
       "      <td>1865</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467</td>\n",
       "      <td>786</td>\n",
       "      <td>...</td>\n",
       "      <td>1507</td>\n",
       "      <td>839</td>\n",
       "      <td>1334</td>\n",
       "      <td>850</td>\n",
       "      <td>1592</td>\n",
       "      <td>764</td>\n",
       "      <td>1372</td>\n",
       "      <td>774</td>\n",
       "      <td>1606</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>201</td>\n",
       "      <td>124</td>\n",
       "      <td>207</td>\n",
       "      <td>117</td>\n",
       "      <td>195</td>\n",
       "      <td>120</td>\n",
       "      <td>217</td>\n",
       "      <td>120</td>\n",
       "      <td>189</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>211</td>\n",
       "      <td>41</td>\n",
       "      <td>204</td>\n",
       "      <td>34</td>\n",
       "      <td>206</td>\n",
       "      <td>45</td>\n",
       "      <td>201</td>\n",
       "      <td>23</td>\n",
       "      <td>205</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>225</td>\n",
       "      <td>39</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>251</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>160</td>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>152</td>\n",
       "      <td>155</td>\n",
       "      <td>150</td>\n",
       "      <td>178</td>\n",
       "      <td>154</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>32</td>\n",
       "      <td>123</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "      <td>117</td>\n",
       "      <td>120</td>\n",
       "      <td>134</td>\n",
       "      <td>132</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_0  y_0   x_1  y_1   x_2  y_2   x_3  y_3   x_4  y_4  ...   x_6  y_6  \\\n",
       "0     632  295   711  254   602  226   798  311   552  239  ...   458  499   \n",
       "1       0    0     0    0     0    0   549  818   585  819  ...   658  878   \n",
       "2       0    0     0    0     0    0     0    0  2613  590  ...  2659  627   \n",
       "3    2095  481  2110  464  2084  471  2136  470  2070  485  ...  2042  548   \n",
       "4       0    0     0    0     0    0     0    0  1467  786  ...  1507  839   \n",
       "..    ...  ...   ...  ...   ...  ...   ...  ...   ...  ...  ...   ...  ...   \n",
       "263   201  124   207  117   195  120   217  120   189  126  ...   189  151   \n",
       "264   211   41   204   34   206   45   201   23   205   49  ...   225   39   \n",
       "265   160  157   167  152   155  150   178  154   147  149  ...     0    0   \n",
       "266    88   82    95   86    93   76     0    0    93   68  ...    68   57   \n",
       "267   120  128   128  127   117  120   134  132   107  116  ...    90  128   \n",
       "\n",
       "      x_7  y_7   x_8  y_8   x_9  y_9  x_10  y_10  \n",
       "0       0    0     0    0   621  517     0     0  \n",
       "1     410  903   754  839   346  848   812   770  \n",
       "2    2421  564  2760  551  2380  502  2824   495  \n",
       "3    2267  453  1937  460  2296  356  1865   346  \n",
       "4    1334  850  1592  764  1372  774  1606   671  \n",
       "..    ...  ...   ...  ...   ...  ...   ...   ...  \n",
       "263     0    0     0    0     0    0     0     0  \n",
       "264   216    2   251   37     0    0   295    30  \n",
       "265     0    0     0    0     0    0     0     0  \n",
       "266    32  123    24   43    18  104     6    38  \n",
       "267     0    0     0    0     0    0     0     0  \n",
       "\n",
       "[268 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataframe generator\n",
    "\n",
    "model = YOLO('yolov8s-pose.pt')\n",
    "\n",
    "image_dir = 'dataset/train/healthy/'\n",
    "\n",
    "position_df = pd.DataFrame()\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        frame = cv2.imread(image_path)\n",
    "\n",
    "        results = model(frame, device = 0)\n",
    "\n",
    "        x_values = []\n",
    "        y_values = []\n",
    "        data={}\n",
    "        img_df = pd.DataFrame()\n",
    "\n",
    "        for result in results:\n",
    "            kpts = result.keypoints\n",
    "            # first 11 keypoints are needed, legs can be ignored\n",
    "            upperbody_kpts = 11\n",
    "\n",
    "            for person in range(len(kpts)):\n",
    "                for kp_index in range(upperbody_kpts):\n",
    "                    keypoint = kpts.xy[person, kp_index]\n",
    "                    x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "\n",
    "                    x_values.append(x)\n",
    "                    y_values.append(y)\n",
    "\n",
    "                for i in range(len(x_values)):\n",
    "                    data[f'x_{i}'] = x_values[i]\n",
    "                    data[f'y_{i}'] = y_values[i]\n",
    "\n",
    "                x_values = []\n",
    "                y_values = []\n",
    "                \n",
    "                temp_df = pd.DataFrame([data])\n",
    "                img_df = pd.concat([img_df, temp_df], ignore_index=True)\n",
    "                temp_df={}    \n",
    "\n",
    "        # per image dataframe concatenated to main\n",
    "        position_df = pd.concat([position_df, img_df], ignore_index=True)\n",
    "\n",
    "display(position_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 person, 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.7ms\n",
      "Speed: 1.4ms preprocess, 10.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9.1ms\n",
      "Speed: 1.1ms preprocess, 9.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.6ms\n",
      "Speed: 1.6ms preprocess, 14.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 person, 11.0ms\n",
      "Speed: 1.5ms preprocess, 11.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.6ms\n",
      "Speed: 1.4ms preprocess, 11.6ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 0.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 320x640 2 persons, 8.5ms\n",
      "Speed: 1.1ms preprocess, 8.5ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 352x640 2 persons, 8.8ms\n",
      "Speed: 1.1ms preprocess, 8.8ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.9ms preprocess, 10.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.5ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 10.6ms\n",
      "Speed: 1.6ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.4ms\n",
      "Speed: 1.5ms preprocess, 14.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 512x640 1 person, 11.5ms\n",
      "Speed: 1.4ms preprocess, 11.5ms inference, 0.7ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 0.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.5ms\n",
      "Speed: 2.2ms preprocess, 13.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 288x640 2 persons, 7.2ms\n",
      "Speed: 0.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.0ms\n",
      "Speed: 1.2ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.1ms\n",
      "Speed: 2.7ms preprocess, 10.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.9ms\n",
      "Speed: 1.2ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.6ms\n",
      "Speed: 1.4ms preprocess, 7.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 416x640 5 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.9ms\n",
      "Speed: 1.3ms preprocess, 12.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.3ms\n",
      "Speed: 1.8ms preprocess, 13.3ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.1ms\n",
      "Speed: 1.1ms preprocess, 10.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.1ms preprocess, 9.7ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 10.4ms\n",
      "Speed: 1.0ms preprocess, 10.4ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 1.1ms preprocess, 10.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.2ms\n",
      "Speed: 1.3ms preprocess, 13.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.4ms\n",
      "Speed: 1.4ms preprocess, 12.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 544x640 1 person, 11.0ms\n",
      "Speed: 1.6ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.9ms\n",
      "Speed: 1.2ms preprocess, 12.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 1.1ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.1ms preprocess, 9.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.1ms\n",
      "Speed: 2.0ms preprocess, 13.1ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.9ms\n",
      "Speed: 1.3ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.9ms\n",
      "Speed: 1.6ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.4ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 17.2ms\n",
      "Speed: 3.1ms preprocess, 17.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 11.0ms\n",
      "Speed: 2.1ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x448 3 persons, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 10.0ms\n",
      "Speed: 1.2ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.2ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 7.3ms\n",
      "Speed: 1.2ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.9ms\n",
      "Speed: 1.4ms preprocess, 12.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 9.4ms\n",
      "Speed: 0.9ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x480 1 person, 11.2ms\n",
      "Speed: 1.9ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.2ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.0ms\n",
      "Speed: 1.1ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.8ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 12.3ms\n",
      "Speed: 1.2ms preprocess, 12.3ms inference, 0.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.4ms\n",
      "Speed: 1.1ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.5ms\n",
      "Speed: 1.0ms preprocess, 11.5ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.7ms\n",
      "Speed: 1.1ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 12.0ms\n",
      "Speed: 1.2ms preprocess, 12.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.4ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 14.7ms\n",
      "Speed: 1.3ms preprocess, 14.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.0ms\n",
      "Speed: 1.2ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 11.4ms\n",
      "Speed: 1.2ms preprocess, 11.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.7ms\n",
      "Speed: 1.1ms preprocess, 10.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.2ms\n",
      "Speed: 1.1ms preprocess, 9.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 2 persons, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x512 1 person, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "\n",
      "0: 640x416 1 person, 9.4ms\n",
      "Speed: 1.0ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 448x640 1 person, 10.3ms\n",
      "Speed: 1.5ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.0ms\n",
      "Speed: 1.6ms preprocess, 13.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.7ms\n",
      "Speed: 1.5ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.0ms\n",
      "Speed: 1.9ms preprocess, 13.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.4ms\n",
      "Speed: 1.4ms preprocess, 10.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 512x640 1 person, 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 1.2ms preprocess, 10.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 1.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.1ms preprocess, 9.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.0ms preprocess, 9.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 1.1ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.8ms\n",
      "Speed: 2.4ms preprocess, 12.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 2 persons, 10.2ms\n",
      "Speed: 1.2ms preprocess, 10.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.1ms\n",
      "Speed: 1.5ms preprocess, 13.1ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.7ms\n",
      "Speed: 1.8ms preprocess, 12.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.7ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.5ms\n",
      "Speed: 1.9ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 9.5ms\n",
      "Speed: 1.1ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 1 person, 11.3ms\n",
      "Speed: 1.1ms preprocess, 11.3ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.3ms\n",
      "Speed: 1.1ms preprocess, 8.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 1.1ms preprocess, 10.2ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.2ms\n",
      "Speed: 1.0ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.5ms\n",
      "Speed: 1.9ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 1 person, 13.4ms\n",
      "Speed: 1.3ms preprocess, 13.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 480x640 2 persons, 17.0ms\n",
      "Speed: 2.8ms preprocess, 17.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.7ms\n",
      "Speed: 1.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 512x640 1 person, 10.6ms\n",
      "Speed: 1.2ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.6ms\n",
      "Speed: 1.1ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.1ms\n",
      "Speed: 1.1ms preprocess, 10.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 3 persons, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 7.3ms\n",
      "Speed: 1.0ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 9 persons, 7.4ms\n",
      "Speed: 1.2ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 384x640 1 person, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 7.2ms\n",
      "Speed: 1.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.1ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 4 persons, 8.0ms\n",
      "Speed: 0.9ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.7ms\n",
      "Speed: 1.2ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 5 persons, 9.6ms\n",
      "Speed: 1.3ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.1ms\n",
      "Speed: 1.3ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.2ms\n",
      "Speed: 1.2ms preprocess, 8.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 352x640 4 persons, 7.6ms\n",
      "Speed: 0.9ms preprocess, 7.6ms inference, 0.9ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 7.8ms\n",
      "Speed: 1.1ms preprocess, 7.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.3ms\n",
      "Speed: 1.2ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 5 persons, 10.3ms\n",
      "Speed: 1.4ms preprocess, 10.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.9ms\n",
      "Speed: 1.0ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 4 persons, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 416x640 1 person, 10.6ms\n",
      "Speed: 1.1ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 448x640 6 persons, 10.1ms\n",
      "Speed: 2.8ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 11 persons, 7.8ms\n",
      "Speed: 1.0ms preprocess, 7.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 2 persons, 9.1ms\n",
      "Speed: 2.6ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 2 persons, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 5 persons, 9.2ms\n",
      "Speed: 1.0ms preprocess, 9.2ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 7 persons, 9.7ms\n",
      "Speed: 1.0ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 5 persons, 8.4ms\n",
      "Speed: 0.9ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 608x640 1 person, 11.7ms\n",
      "Speed: 1.2ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 320x640 1 person, 7.2ms\n",
      "Speed: 0.9ms preprocess, 7.2ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 480x640 5 persons, 9.9ms\n",
      "Speed: 1.5ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 384x640 1 person, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 256x640 6 persons, 6.0ms\n",
      "Speed: 0.9ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 256, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.6ms\n",
      "Speed: 1.4ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.6ms\n",
      "Speed: 1.1ms preprocess, 9.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 480x640 1 person, 10.3ms\n",
      "Speed: 1.3ms preprocess, 10.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 448x640 1 person, 10.0ms\n",
      "Speed: 1.1ms preprocess, 10.0ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 448x640 1 person, 9.9ms\n",
      "Speed: 1.0ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 480x640 1 person, 10.1ms\n",
      "Speed: 4.1ms preprocess, 10.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 640x640 1 person, 12.4ms\n",
      "Speed: 2.9ms preprocess, 12.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x448 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x448 1 person, 10.9ms\n",
      "Speed: 1.8ms preprocess, 10.9ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 1.1ms preprocess, 11.2ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 352x640 1 person, 8.8ms\n",
      "Speed: 1.0ms preprocess, 8.8ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 448x640 18 persons, 11.8ms\n",
      "Speed: 1.2ms preprocess, 11.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 576x640 1 person, 12.7ms\n",
      "Speed: 1.9ms preprocess, 12.7ms inference, 0.9ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 320x640 1 person, 9.3ms\n",
      "Speed: 1.1ms preprocess, 9.3ms inference, 0.9ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 448x640 1 person, 11.2ms\n",
      "Speed: 0.9ms preprocess, 11.2ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x544 1 person, 12.2ms\n",
      "Speed: 1.2ms preprocess, 12.2ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 544)\n",
      "\n",
      "0: 448x640 7 persons, 11.5ms\n",
      "Speed: 2.5ms preprocess, 11.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x416 14 persons, 11.4ms\n",
      "Speed: 1.3ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 288x640 12 persons, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 288, 640)\n",
      "\n",
      "0: 480x640 6 persons, 11.9ms\n",
      "Speed: 1.2ms preprocess, 11.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 544x640 5 persons, 12.3ms\n",
      "Speed: 1.7ms preprocess, 12.3ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 352x640 14 persons, 8.6ms\n",
      "Speed: 1.1ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 640x448 1 person, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "\n",
      "0: 352x640 12 persons, 8.6ms\n",
      "Speed: 1.1ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 352x640 5 persons, 8.1ms\n",
      "Speed: 0.9ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>...</th>\n",
       "      <th>y_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>y_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>y_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>y_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>y_10</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>348</td>\n",
       "      <td>678</td>\n",
       "      <td>499</td>\n",
       "      <td>384</td>\n",
       "      <td>509</td>\n",
       "      <td>575</td>\n",
       "      <td>194</td>\n",
       "      <td>509</td>\n",
       "      <td>184</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>235</td>\n",
       "      <td>528</td>\n",
       "      <td>190</td>\n",
       "      <td>450</td>\n",
       "      <td>188</td>\n",
       "      <td>586</td>\n",
       "      <td>195</td>\n",
       "      <td>399</td>\n",
       "      <td>188</td>\n",
       "      <td>...</td>\n",
       "      <td>393</td>\n",
       "      <td>822</td>\n",
       "      <td>479</td>\n",
       "      <td>56</td>\n",
       "      <td>501</td>\n",
       "      <td>736</td>\n",
       "      <td>267</td>\n",
       "      <td>251</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>166</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>187</td>\n",
       "      <td>74</td>\n",
       "      <td>192</td>\n",
       "      <td>67</td>\n",
       "      <td>153</td>\n",
       "      <td>78</td>\n",
       "      <td>156</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>111</td>\n",
       "      <td>145</td>\n",
       "      <td>87</td>\n",
       "      <td>106</td>\n",
       "      <td>88</td>\n",
       "      <td>166</td>\n",
       "      <td>106</td>\n",
       "      <td>75</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>201</td>\n",
       "      <td>124</td>\n",
       "      <td>207</td>\n",
       "      <td>117</td>\n",
       "      <td>195</td>\n",
       "      <td>120</td>\n",
       "      <td>217</td>\n",
       "      <td>120</td>\n",
       "      <td>189</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>211</td>\n",
       "      <td>41</td>\n",
       "      <td>204</td>\n",
       "      <td>34</td>\n",
       "      <td>206</td>\n",
       "      <td>45</td>\n",
       "      <td>201</td>\n",
       "      <td>23</td>\n",
       "      <td>205</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>251</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>160</td>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>152</td>\n",
       "      <td>155</td>\n",
       "      <td>150</td>\n",
       "      <td>178</td>\n",
       "      <td>154</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>57</td>\n",
       "      <td>32</td>\n",
       "      <td>123</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>104</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "      <td>117</td>\n",
       "      <td>120</td>\n",
       "      <td>134</td>\n",
       "      <td>132</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_0  y_0  x_1  y_1  x_2  y_2  x_3  y_3  x_4  y_4  ...  y_6  x_7  y_7  \\\n",
       "0    478  163    0    0  440  140    0    0  334  189  ...  348  678  499   \n",
       "1    488  235  528  190  450  188  586  195  399  188  ...  393  822  479   \n",
       "2    169   43  166   38    0    0  171   27    0    0  ...   33  187   74   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4    130  111  145   87  106   88  166  106   75  111  ...  227    0    0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "423  201  124  207  117  195  120  217  120  189  126  ...  151    0    0   \n",
       "424  211   41  204   34  206   45  201   23  205   49  ...   39  216    2   \n",
       "425  160  157  167  152  155  150  178  154  147  149  ...    0    0    0   \n",
       "426   88   82   95   86   93   76    0    0   93   68  ...   57   32  123   \n",
       "427  120  128  128  127  117  120  134  132  107  116  ...  128    0    0   \n",
       "\n",
       "     x_8  y_8  x_9  y_9  x_10  y_10  class  \n",
       "0    384  509  575  194   509   184      1  \n",
       "1     56  501  736  267   251   252      1  \n",
       "2    192   67  153   78   156    77      1  \n",
       "3      0    0    0    0     0     0      1  \n",
       "4      0    0    0    0     0     0      1  \n",
       "..   ...  ...  ...  ...   ...   ...    ...  \n",
       "423    0    0    0    0     0     0      0  \n",
       "424  251   37    0    0   295    30      0  \n",
       "425    0    0    0    0     0     0      0  \n",
       "426   24   43   18  104     6    38      0  \n",
       "427    0    0    0    0     0     0      0  \n",
       "\n",
       "[428 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class adder\n",
    "\n",
    "model = YOLO('yolov8s-pose.pt')\n",
    "\n",
    "main_dir = 'dataset/train/'\n",
    "\n",
    "position_df = pd.DataFrame()\n",
    "\n",
    "for class_name in os.listdir(main_dir):\n",
    "    class_path = os.path.join(main_dir, class_name)\n",
    "    \n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(class_path, filename)\n",
    "                frame = cv2.imread(image_path)\n",
    "\n",
    "                results = model(frame, device=0)\n",
    "\n",
    "                x_values = []\n",
    "                y_values = []\n",
    "                data = {}\n",
    "                img_df = pd.DataFrame()\n",
    "\n",
    "                for result in results:\n",
    "                    kpts = result.keypoints\n",
    "                    upperbody_kpts = 11\n",
    "\n",
    "                    for person in range(len(kpts)):\n",
    "                        for kp_index in range(upperbody_kpts):\n",
    "                            keypoint = kpts.xy[person, kp_index]\n",
    "                            x, y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "\n",
    "                            x_values.append(x)\n",
    "                            y_values.append(y)\n",
    "\n",
    "                        for i in range(len(x_values)):\n",
    "                            data[f'x_{i}'] = x_values[i]\n",
    "                            data[f'y_{i}'] = y_values[i]\n",
    "\n",
    "                        x_values = []\n",
    "                        y_values = []\n",
    "\n",
    "                        # Add a column for class (0 for \"healthy\", 1 for \"risk\")\n",
    "                        data['class'] = 0 if class_name == 'healthy' else 1\n",
    "\n",
    "                        temp_df = pd.DataFrame([data])\n",
    "                        img_df = pd.concat([img_df, temp_df], ignore_index=True)\n",
    "                        temp_df = {}\n",
    "\n",
    "                # Concatenate per-image DataFrame to the main DataFrame\n",
    "                position_df = pd.concat([position_df, img_df], ignore_index=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(position_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>y_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>y_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>y_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>y_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>y_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>478</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334</td>\n",
       "      <td>189</td>\n",
       "      <td>478</td>\n",
       "      <td>342</td>\n",
       "      <td>249</td>\n",
       "      <td>348</td>\n",
       "      <td>678</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>235</td>\n",
       "      <td>528</td>\n",
       "      <td>190</td>\n",
       "      <td>450</td>\n",
       "      <td>188</td>\n",
       "      <td>586</td>\n",
       "      <td>195</td>\n",
       "      <td>399</td>\n",
       "      <td>188</td>\n",
       "      <td>704</td>\n",
       "      <td>402</td>\n",
       "      <td>267</td>\n",
       "      <td>393</td>\n",
       "      <td>822</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>166</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>30</td>\n",
       "      <td>205</td>\n",
       "      <td>33</td>\n",
       "      <td>187</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>111</td>\n",
       "      <td>145</td>\n",
       "      <td>87</td>\n",
       "      <td>106</td>\n",
       "      <td>88</td>\n",
       "      <td>166</td>\n",
       "      <td>106</td>\n",
       "      <td>75</td>\n",
       "      <td>111</td>\n",
       "      <td>214</td>\n",
       "      <td>225</td>\n",
       "      <td>27</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>201</td>\n",
       "      <td>124</td>\n",
       "      <td>207</td>\n",
       "      <td>117</td>\n",
       "      <td>195</td>\n",
       "      <td>120</td>\n",
       "      <td>217</td>\n",
       "      <td>120</td>\n",
       "      <td>189</td>\n",
       "      <td>126</td>\n",
       "      <td>240</td>\n",
       "      <td>139</td>\n",
       "      <td>189</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>211</td>\n",
       "      <td>41</td>\n",
       "      <td>204</td>\n",
       "      <td>34</td>\n",
       "      <td>206</td>\n",
       "      <td>45</td>\n",
       "      <td>201</td>\n",
       "      <td>23</td>\n",
       "      <td>205</td>\n",
       "      <td>49</td>\n",
       "      <td>213</td>\n",
       "      <td>7</td>\n",
       "      <td>225</td>\n",
       "      <td>39</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>160</td>\n",
       "      <td>157</td>\n",
       "      <td>167</td>\n",
       "      <td>152</td>\n",
       "      <td>155</td>\n",
       "      <td>150</td>\n",
       "      <td>178</td>\n",
       "      <td>154</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>95</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>105</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>32</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "      <td>117</td>\n",
       "      <td>120</td>\n",
       "      <td>134</td>\n",
       "      <td>132</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>132</td>\n",
       "      <td>152</td>\n",
       "      <td>90</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     x_0  y_0  x_1  y_1  x_2  y_2  x_3  y_3  x_4  y_4  x_5  y_5  x_6  y_6  \\\n",
       "0    478  163    0    0  440  140    0    0  334  189  478  342  249  348   \n",
       "1    488  235  528  190  450  188  586  195  399  188  704  402  267  393   \n",
       "2    169   43  166   38    0    0  171   27    0    0  198   30  205   33   \n",
       "3      0    0    0    0    0    0    0    0    0    0    0    0    0    0   \n",
       "4    130  111  145   87  106   88  166  106   75  111  214  225   27  227   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "423  201  124  207  117  195  120  217  120  189  126  240  139  189  151   \n",
       "424  211   41  204   34  206   45  201   23  205   49  213    7  225   39   \n",
       "425  160  157  167  152  155  150  178  154  147  149    0    0    0    0   \n",
       "426   88   82   95   86   93   76    0    0   93   68   70  105   68   57   \n",
       "427  120  128  128  127  117  120  134  132  107  116  132  152   90  128   \n",
       "\n",
       "     x_7  y_7  \n",
       "0    678  499  \n",
       "1    822  479  \n",
       "2    187   74  \n",
       "3      0    0  \n",
       "4      0    0  \n",
       "..   ...  ...  \n",
       "423    0    0  \n",
       "424  216    2  \n",
       "425    0    0  \n",
       "426   32  123  \n",
       "427    0    0  \n",
       "\n",
       "[428 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "235",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 235",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m, in \u001b[0;36mBodyLandmarksDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Code/DataScience/env/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 235"
     ]
    }
   ],
   "source": [
    "# model trainer\n",
    "\n",
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class BodyLandmarksDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(idx)\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "def get_class_list(df):\n",
    "    # Ensure the DataFrame is not empty\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Get the last column name\n",
    "    last_attribute = df.columns[-1]\n",
    "\n",
    "    # Extract the values of the last attribute and convert to a list\n",
    "    values_list = df[last_attribute].tolist()\n",
    "\n",
    "    return values_list\n",
    "\n",
    "# 11x2 (x, y) body landmarks (22 features) for each sample\n",
    "y = get_class_list(position_df)  # Output labels\n",
    "position_df.drop(position_df.columns[-1], axis=1, inplace=True)\n",
    "display(position_df)\n",
    "X = position_df  # Position (pandas dataframe)\n",
    "\n",
    "\n",
    "\n",
    "# Split data into train and test sets using PyTorch's native functionality\n",
    "dataset = BodyLandmarksDataset(X, y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = 22  # 11x2 coordinates\n",
    "output_size = 2  # Number of output classes\n",
    "hidden_size = 64\n",
    "\n",
    "model = EmotionModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        print(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.view(-1, input_size))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.view(-1, input_size))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
